// src/api.rs
// Axum-based API router for transaction submit + basic checks
use crate::sled_storage::{open_db, get_str};

use axum::extract::{ConnectInfo, Extension, Path};
use axum::http::{Request, StatusCode};
use axum::middleware::{self, Next};
use axum::response::{IntoResponse, Json, Response};
use axum::routing::{get, post};
use axum::Router;
use serde::{Deserialize, Serialize};
use serde_json::Value as JsonValue;
use sqlx::{PgPool, Row};
use std::collections::HashMap;
use std::net::SocketAddr;
use std::sync::{Arc, Mutex};
use std::time::{Duration, Instant};
use thiserror::Error;
use tracing::{error, info, warn};
use uuid::Uuid;

type ApiResult = Result<Response, ApiError>;

/// Simple token bucket rate limiter.
/// Tracks requests per IP address with a sliding window.
#[derive(Clone)]
struct RateLimiter {
    // Map of IP address -> (request_count, window_start_time)
    buckets: Arc<Mutex<HashMap<String, (u32, Instant)>>>,
    max_requests: u32,
    window_duration: Duration,
}

impl RateLimiter {
    fn new(max_requests: u32, window_secs: u64) -> Self {
        Self {
            buckets: Arc::new(Mutex::new(HashMap::new())),
            max_requests,
            window_duration: Duration::from_secs(window_secs),
        }
    }

    /// Check if a request from the given IP should be allowed.
    /// Returns true if allowed, false if rate limit exceeded.
    fn check_rate_limit(&self, ip: &str) -> bool {
        // Handle mutex poisoning gracefully - recover the data even if poisoned
        let mut buckets = self.buckets.lock().unwrap_or_else(|poisoned| {
            warn!("‚ö†Ô∏è  Rate limiter mutex poisoned - recovering data");
            poisoned.into_inner()
        });
        let now = Instant::now();

        let entry = buckets.entry(ip.to_string()).or_insert((0, now));
        let (count, window_start) = entry;

        // Check if we're in a new window
        if now.duration_since(*window_start) > self.window_duration {
            // Reset window
            *count = 1;
            *window_start = now;
            true
        } else if *count < self.max_requests {
            // Within limit
            *count += 1;
            true
        } else {
            // Rate limit exceeded
            false
        }
    }

    /// Cleanup old entries (optional, prevents memory growth)
    #[allow(dead_code)]
    fn cleanup_old_entries(&self) {
        let mut buckets = self.buckets.lock().unwrap_or_else(|poisoned| {
            warn!("‚ö†Ô∏è  Rate limiter mutex poisoned during cleanup - recovering data");
            poisoned.into_inner()
        });
        let now = Instant::now();
        buckets.retain(|_, (_, window_start)| {
            now.duration_since(*window_start) < self.window_duration * 2
        });
    }
}

#[derive(Debug, Deserialize)]
pub struct IncomingTxn {
    pub tx_hash: String,
    pub sender: String,
    pub recipient: String,
    pub payload: JsonValue, // full signed payload from client
    pub signature: Option<String>,        // optional meta
    pub idempotency_key: Option<String>,  // optional client-supplied idempotency key
    pub nonce: Option<i64>,               // optional account nonce
}

#[derive(Debug, Serialize)]
struct TxSubmitResponse {
    tx_id: Uuid,
    status: &'static str,
}

#[derive(Debug, Error)]
enum ApiError {
    #[error("database error")]
    Db(sqlx::Error),

    #[error("duplicate transaction")]
    Duplicate,

    #[error("bad request: {0}")]
    BadRequest(String),
}

impl From<sqlx::Error> for ApiError {
    fn from(e: sqlx::Error) -> Self {
        ApiError::Db(e)
    }
}

impl IntoResponse for ApiError {
    fn into_response(self) -> axum::response::Response {
        let (status, body) = match &self {
            ApiError::Db(e) => {
                error!("DB error: {:?}", e);
                (StatusCode::INTERNAL_SERVER_ERROR, "database error".to_string())
            }
            ApiError::Duplicate => (StatusCode::CONFLICT, "duplicate transaction".to_string()),
            ApiError::BadRequest(msg) => (StatusCode::BAD_REQUEST, msg.clone()),
        };
        let body_json = serde_json::json!({ "error": body });
        (status, Json(body_json)).into_response()
    }
}

///////////////////////////////////////////////////////////////////////////
// POST /tx/submit
///////////////////////////////////////////////////////////////////////////
async fn submit_tx(
    Extension(db_pool): Extension<PgPool>,
    Extension(batch_writer): Extension<Arc<crate::batch_writer::BatchWriter>>,
    Json(incoming): Json<IncomingTxn>,
) -> Result<impl IntoResponse, ApiError> {
    // Basic validation
    if incoming.tx_hash.trim().is_empty() {
        return Err(ApiError::BadRequest("tx_hash required".into()));
    }
    if incoming.sender.trim().is_empty() || incoming.recipient.trim().is_empty() {
        return Err(ApiError::BadRequest("sender and recipient are required".into()));
    }

    // Check for duplicate by tx_hash or idempotency_key
    if let Some(_) = sqlx::query_scalar::<_, String>(
        "SELECT tx_hash FROM tx_index WHERE tx_hash = $1",
    )
    .bind(&incoming.tx_hash)
    .fetch_optional(&db_pool)
    .await?
    {
        info!("duplicate tx_hash submitted: {}", &incoming.tx_hash);
        return Err(ApiError::Duplicate);
    }

    // Mandatory signature verification: if client included public_key/signature in payload,
    // verify signature over tx_hash using Ed25519 cryptographic verification.
    // SECURITY: No fallback - only real cryptographic verification accepted.
    if let Some(pubkey) = incoming.payload.get("public_key").and_then(|v| v.as_str()) {
        if let Some(sig) = incoming.payload.get("signature").and_then(|v| v.as_str()) {
            let message = incoming.tx_hash.as_bytes();
            // Always use real cryptographic verification - no shortcuts
            let ok = crate::crypto::verify_ed25519_hex(pubkey, sig, message);
            if !ok {
                return Err(ApiError::BadRequest("signature invalid".into()));
            }
        }
    }

    // Optionally check idempotency_key uniqueness
    if let Some(key) = &incoming.idempotency_key {
        if let Some(existing_tx_id) = sqlx::query_scalar::<_, Uuid>(
            "SELECT tx_id FROM transactions WHERE idempotency_key = $1",
        )
        .bind(key)
        .fetch_optional(&db_pool)
        .await?
        {
            // Return existing tx_id back to caller (idempotent)
            info!("idempotency key already present, returning existing tx_id");
            let resp = TxSubmitResponse {
                tx_id: existing_tx_id,
                status: "pending",
            };
            return Ok((StatusCode::OK, Json(resp)));
        }
    }

    // TPS OPTIMIZATION: Queue transaction for batch processing instead of synchronous DB writes
    // This enables 20k-50k TPS by batching writes every 100ms or 500 transactions
    let tx_id = Uuid::new_v4();

    // Extract fields for batch writer
    let amount = incoming.payload.get("amount")
        .and_then(|v| v.as_u64())
        .unwrap_or(0);
    let fee = incoming.payload.get("fee")
        .and_then(|v| v.as_u64())
        .unwrap_or(0);
    let public_key = incoming.payload.get("public_key")
        .and_then(|v| v.as_str())
        .unwrap_or("")
        .to_string();

    let pending_tx = crate::batch_writer::PendingTransaction {
        tx_id,
        tx_hash: incoming.tx_hash.clone(),
        sender: incoming.sender.clone(),
        recipient: incoming.recipient.clone(),
        payload: incoming.payload.clone(),
        signature: incoming.signature.clone(),
        amount,
        fee,
        public_key,
    };

    // Submit to batch writer (non-blocking, returns immediately)
    if let Err(e) = batch_writer.submit(pending_tx).await {
        return Err(ApiError::BadRequest(format!("Failed to queue transaction: {}", e)));
    }

    info!("‚úÖ Queued tx {} for batch processing (sender: {})", tx_id, &incoming.sender);

    let resp = TxSubmitResponse {
        tx_id,
        status: "pending",
    };
    Ok((StatusCode::ACCEPTED, Json(resp)))
}

///////////////////////////////////////////////////////////////////////////
// GET /mempool
///////////////////////////////////////////////////////////////////////////
async fn get_mempool(Extension(db_pool): Extension<PgPool>) -> ApiResult {
    // cast received_at to text so sqlx doesn't try to map to a non-serializable type
    let rows = sqlx::query(
        r#"
        SELECT tx_id::text AS tx_id, tx_hash, payload::text AS payload_json, received_at::text AS received_at
        FROM mempool_entries
        ORDER BY received_at DESC
        LIMIT 100
        "#,
    )
    .fetch_all(&db_pool)
    .await
    .map_err(ApiError::Db)?;

    let mut out: Vec<JsonValue> = Vec::new();
    for r in rows {
        let tx_id: String = r.try_get("tx_id").map_err(ApiError::Db)?;
        let tx_hash: Option<String> = r.try_get("tx_hash").map_err(ApiError::Db)?;
        let payload_json: Option<String> = r.try_get("payload_json").map_err(ApiError::Db)?;
        let received_at: Option<String> = r.try_get("received_at").map_err(ApiError::Db)?;

        let payload_val: JsonValue = match payload_json {
            Some(s) => serde_json::from_str(&s).unwrap_or(JsonValue::Null),
            None => JsonValue::Null,
        };

        out.push(serde_json::json!({
            "tx_id": tx_id,
            "tx_hash": tx_hash,
            "payload": payload_val,
            "received_at": received_at,
        }));
    }

    Ok((StatusCode::OK, Json(out)).into_response())
}

///////////////////////////////////////////////////////////////////////////
// GET /tx/:id  (tries id as uuid, falls back to tx_hash lookup)
// GET /tx/hash/:hash
///////////////////////////////////////////////////////////////////////////
async fn get_tx_by_id_or_hash(
    Path(id): Path<String>,
    Extension(db_pool): Extension<PgPool>,
) -> ApiResult {
    // if it looks like a UUID, try uuid lookup
    if id.contains('-') {
        if let Ok(uuid) = Uuid::parse_str(&id) {
            return get_tx_by_id_inner(uuid, &db_pool).await;
        }
    }
    // otherwise fall back to hash lookup
    get_tx_by_hash_inner(&id, &db_pool).await
}

async fn get_tx_by_hash(
    Path(hash): Path<String>,
    Extension(db_pool): Extension<PgPool>,
) -> ApiResult {
    get_tx_by_hash_inner(&hash, &db_pool).await
}

async fn get_tx_by_id_inner(uuid: Uuid, pool: &PgPool) -> ApiResult {
    let row_opt = sqlx::query(
        r#"
        SELECT tx_id::text AS tx_id, tx_hash, sender, recipient, payload::text AS payload_json, status, idempotency_key, nonce, created_at::text AS created_at
        FROM transactions WHERE tx_id = $1
        "#,
    )
    .bind(uuid)
    .fetch_optional(pool)
    .await
    .map_err(ApiError::Db)?;

    if let Some(row) = row_opt {
        let tx_id: String = row.try_get("tx_id").map_err(ApiError::Db)?;
        let tx_hash: Option<String> = row.try_get("tx_hash").map_err(ApiError::Db)?;
        let sender: Option<String> = row.try_get("sender").map_err(ApiError::Db)?;
        let recipient: Option<String> = row.try_get("recipient").map_err(ApiError::Db)?;
        let payload_json: Option<String> = row.try_get("payload_json").map_err(ApiError::Db)?;
        let status: Option<String> = row.try_get("status").map_err(ApiError::Db)?;
        let idempotency_key: Option<String> = row.try_get("idempotency_key").map_err(ApiError::Db)?;
        let nonce: Option<i64> = row.try_get("nonce").map_err(ApiError::Db)?;
        let created_at: Option<String> = row.try_get("created_at").map_err(ApiError::Db)?;

        let payload_val: JsonValue = match payload_json {
            Some(s) => serde_json::from_str(&s).unwrap_or(JsonValue::Null),
            None => JsonValue::Null,
        };

        return Ok((StatusCode::OK, Json(serde_json::json!({
            "tx_id": tx_id,
            "tx_hash": tx_hash,
            "sender": sender,
            "recipient": recipient,
            "payload": payload_val,
            "status": status,
            "idempotency_key": idempotency_key,
            "nonce": nonce,
            "created_at": created_at,
        }))).into_response());
    }

    Ok((StatusCode::NOT_FOUND, Json(serde_json::json!({ "error": "tx not found" }))).into_response())
}

async fn get_tx_by_hash_inner(hash: &str, pool: &PgPool) -> ApiResult {
    let row_opt = sqlx::query(
        r#"
        SELECT tx_id::text AS tx_id, tx_hash, sender, recipient, payload::text AS payload_json, status, idempotency_key, nonce, created_at::text AS created_at
        FROM transactions WHERE tx_hash = $1
        "#,
    )
    .bind(hash)
    .fetch_optional(pool)
    .await
    .map_err(ApiError::Db)?;

    if let Some(row) = row_opt {
        let tx_id: String = row.try_get("tx_id").map_err(ApiError::Db)?;
        let tx_hash: Option<String> = row.try_get("tx_hash").map_err(ApiError::Db)?;
        let sender: Option<String> = row.try_get("sender").map_err(ApiError::Db)?;
        let recipient: Option<String> = row.try_get("recipient").map_err(ApiError::Db)?;
        let payload_json: Option<String> = row.try_get("payload_json").map_err(ApiError::Db)?;
        let status: Option<String> = row.try_get("status").map_err(ApiError::Db)?;
        let idempotency_key: Option<String> = row.try_get("idempotency_key").map_err(ApiError::Db)?;
        let nonce: Option<i64> = row.try_get("nonce").map_err(ApiError::Db)?;
        let created_at: Option<String> = row.try_get("created_at").map_err(ApiError::Db)?;

        let payload_val: JsonValue = match payload_json {
            Some(s) => serde_json::from_str(&s).unwrap_or(JsonValue::Null),
            None => JsonValue::Null,
        };

        return Ok((StatusCode::OK, Json(serde_json::json!({
            "tx_id": tx_id,
            "tx_hash": tx_hash,
            "sender": sender,
            "recipient": recipient,
            "payload": payload_val,
            "status": status,
            "idempotency_key": idempotency_key,
            "nonce": nonce,
            "created_at": created_at,
        }))).into_response());
    }

    Ok((StatusCode::NOT_FOUND, Json(serde_json::json!({ "error": "tx not found" }))).into_response())
}

///////////////////////////////////////////////////////////////////////////
// GET /proof/:tx  (lookup tx_index)
///////////////////////////////////////////////////////////////////////////
async fn get_proof_by_tx(
    Path(tx): Path<String>,
    Extension(db_pool): Extension<PgPool>,
) -> ApiResult {
    let row_opt = sqlx::query(
        r#"
        SELECT tx_hash, tx_id::text AS tx_id, block_id::text AS block_id, created_at::text AS created_at
        FROM tx_index WHERE tx_hash = $1
        "#,
    )
    .bind(&tx)
    .fetch_optional(&db_pool)
    .await
    .map_err(ApiError::Db)?;

    if let Some(row) = row_opt {
        let tx_hash: Option<String> = row.try_get("tx_hash").map_err(ApiError::Db)?;
        let tx_id: Option<String> = row.try_get("tx_id").map_err(ApiError::Db)?;
        let block_id: Option<String> = row.try_get("block_id").map_err(ApiError::Db)?;
        let created_at: Option<String> = row.try_get("created_at").map_err(ApiError::Db)?;

        // Generate Merkle proof if transaction is in a block
        let proof = if let Some(ref bid) = block_id {
            // Query all transactions in this block (ordered by creation)
            let block_txs = sqlx::query(
                r#"
                SELECT tx_hash FROM tx_index
                WHERE block_id = $1
                ORDER BY created_at ASC
                "#
            )
            .bind(bid)
            .fetch_all(&db_pool)
            .await
            .map_err(ApiError::Db)?;

            if !block_txs.is_empty() {
                // Extract transaction hashes
                let tx_hashes: Vec<String> = block_txs
                    .iter()
                    .filter_map(|r| r.try_get::<String, _>("tx_hash").ok())
                    .collect();

                // Find index of requested transaction
                let tx_index = tx_hashes.iter().position(|h| Some(h) == tx_hash.as_ref());

                if let Some(idx) = tx_index {
                    // Build Merkle tree and generate proof
                    let tree = crate::merkle::MerkleTree::from_hashes(&tx_hashes);
                    let inclusion_proof = tree.proof_for_index(idx);
                    let root = tree.root_hex();

                    serde_json::json!({
                        "root": root,
                        "index": idx,
                        "path": inclusion_proof.iter().map(|(hash, is_left)| {
                            serde_json::json!({"sibling": hash, "is_left": is_left})
                        }).collect::<Vec<_>>()
                    })
                } else {
                    serde_json::json!({"error": "transaction not found in block"})
                }
            } else {
                serde_json::json!({"error": "block has no transactions"})
            }
        } else {
            serde_json::json!({"info": "transaction not yet included in block"})
        };

        let payload = serde_json::json!({
            "tx_hash": tx_hash,
            "tx_id": tx_id,
            "block_id": block_id,
            "created_at": created_at,
            "proof": proof
        });

        return Ok((StatusCode::OK, Json(payload)).into_response());
    }

    Ok((StatusCode::NOT_FOUND, Json(serde_json::json!({ "error": "tx not found" }))).into_response())
}

///////////////////////////////////////////////////////////////////////////
// GET /block/:id  -- placeholder; adapt to your block schema if needed
///////////////////////////////////////////////////////////////////////////
async fn get_block_by_id(
    Path(id): Path<String>,
    Extension(_db_pool): Extension<PgPool>,
) -> ApiResult {
    // Try to read block from RocksDB (if you persist blocks under key "block:<id>")
    let rocks_path = std::env::var("ROCKSDB_PATH").unwrap_or_else(|_| "sled_data".into());

    // open_db returns the DB handle (not a Result) ‚Äî call it directly
    let db = open_db(&rocks_path);

    // attempt to read the key
    let key = format!("block:{}", id);
    match get_str::<serde_json::Value>(&db, &key) {
        Ok(Some(val)) => {
            return Ok((StatusCode::OK, Json(val)).into_response());
        }
        Ok(None) => {
            // not found
            return Ok((StatusCode::NOT_FOUND, Json(serde_json::json!({ "error": "block not found" }))).into_response());
        }
        Err(e) => {
            eprintln!("get_block_by_id: rocksdb read error: {:?}", e);
            return Ok((StatusCode::INTERNAL_SERVER_ERROR, Json(serde_json::json!({
                "error": "rocksdb read error"
            }))).into_response());
        }
    }
}

///////////////////////////////////////////////////////////////////////////
// GET /health - Basic health check
///////////////////////////////////////////////////////////////////////////
async fn health(Extension(db_pool): Extension<PgPool>) -> ApiResult {
    match sqlx::query_scalar::<_, i32>("SELECT 1").fetch_one(&db_pool).await {
        Ok(_) => Ok((StatusCode::OK, Json(serde_json::json!({"status":"ok"}))).into_response()),
        Err(e) => {
            eprintln!("health: db error: {:?}", e);
            Ok((StatusCode::SERVICE_UNAVAILABLE, Json(serde_json::json!({"status":"db-unavailable"}))).into_response())
        }
    }
}

///////////////////////////////////////////////////////////////////////////
// GET /health/detailed - Detailed health diagnostics
///////////////////////////////////////////////////////////////////////////
async fn health_detailed(
    Extension(db_pool): Extension<PgPool>,
    Extension(peer_store): Extension<crate::network::PeerStore>,
) -> ApiResult {
    let mut health_status = serde_json::json!({
        "status": "healthy",
        "timestamp": chrono::Utc::now().to_rfc3339(),
        "checks": {}
    });

    let checks = health_status["checks"].as_object_mut().unwrap();

    // Check 1: Database connectivity
    let db_healthy = match sqlx::query_scalar::<_, i32>("SELECT 1").fetch_one(&db_pool).await {
        Ok(_) => {
            checks.insert("database".to_string(), serde_json::json!({
                "status": "healthy",
                "message": "Database connection OK"
            }));
            true
        }
        Err(e) => {
            checks.insert("database".to_string(), serde_json::json!({
                "status": "unhealthy",
                "message": format!("Database error: {}", e)
            }));
            false
        }
    };

    // Check 2: Database pool statistics
    checks.insert("database_pool".to_string(), serde_json::json!({
        "status": "info",
        "connections": db_pool.size(),
        "idle_connections": db_pool.num_idle(),
    }));

    // Check 3: Mempool status
    let mempool_status = match db_pool.acquire().await {
        Ok(mut conn) => {
            match sqlx::query_scalar::<_, i64>("SELECT COUNT(*) FROM mempool_entries")
                .fetch_one(&mut *conn)
                .await
            {
                Ok(count) => {
                    checks.insert("mempool".to_string(), serde_json::json!({
                        "status": "healthy",
                        "pending_transactions": count
                    }));
                    true
                }
                Err(e) => {
                    checks.insert("mempool".to_string(), serde_json::json!({
                        "status": "warning",
                        "message": format!("Could not query mempool: {}", e)
                    }));
                    true // Not critical
                }
            }
        }
        Err(e) => {
            checks.insert("mempool".to_string(), serde_json::json!({
                "status": "warning",
                "message": format!("Could not acquire connection: {}", e)
            }));
            true // Not critical
        }
    };

    // Check 4: Peer connectivity
    let peer_count = {
        let store = peer_store.lock().await;
        store.len()
    };

    checks.insert("peers".to_string(), serde_json::json!({
        "status": if peer_count > 0 { "healthy" } else { "warning" },
        "connected_peers": peer_count,
        "message": if peer_count == 0 { "No peers connected" } else { "Peers connected" }
    }));

    // Check 5: TLS configuration
    let tls_enabled = std::env::var("TLS_CERT_PATH").is_ok() && std::env::var("TLS_KEY_PATH").is_ok();
    checks.insert("tls".to_string(), serde_json::json!({
        "status": "info",
        "enabled": tls_enabled,
        "message": if tls_enabled { "TLS enabled" } else { "TLS not configured (HTTP only)" }
    }));

    // Check 6: Authentication status (DECENTRALIZED: signature-only)
    checks.insert("authentication".to_string(), serde_json::json!({
        "status": "info",
        "enabled": true,
        "message": "Signature-based authentication (decentralized, no API keys)"
    }));

    // Overall status
    let overall_healthy = db_healthy && mempool_status;
    let status_code = if overall_healthy {
        StatusCode::OK
    } else {
        StatusCode::SERVICE_UNAVAILABLE
    };

    health_status["status"] = if overall_healthy {
        serde_json::json!("healthy")
    } else {
        serde_json::json!("unhealthy")
    };

    Ok((status_code, Json(health_status)).into_response())
}

/// GET /peers - returns the runtime peer store as JSON array (full metadata)
async fn get_peers(
    Extension(peer_store): Extension<crate::network::PeerStore>
) -> Result<impl IntoResponse, ApiError> {
    let store = peer_store.lock().await;
    let out: Vec<_> = store.iter().map(|e| {
        serde_json::json!({
            "addr": e.addr,
            "last_seen": e.last_seen_unix,
            "failures": e.failures,
            "banned_until": e.banned_until_unix,
        })
    }).collect();
    Ok((StatusCode::OK, Json(out)))
}

#[allow(dead_code)]
fn verify_signature(_payload: &JsonValue, _signature: Option<String>) -> Result<(), String> {
    // placeholder for future more complex verification
    Ok(())
}

/// Build router for this microservice (call from main)
/// Accepts the runtime PeerStore so /peers can show discovered peers with metadata.
/// API Key authentication middleware.
///
/// DECENTRALIZATION UPDATE: API keys removed for truly decentralized operation.
/// Authentication is now handled per-transaction via Ed25519 signatures.
/// This middleware allows all requests through; signature verification happens at transaction level.
pub async fn auth_middleware<B>(req: Request<B>, next: Next<B>) -> Result<Response, StatusCode> {
    // DECENTRALIZATION: No API keys - use signature-only auth
    // All requests are allowed through; authentication happens at transaction level via signatures
    Ok(next.run(req).await)
}

/// Request logging middleware.
///
/// Logs all HTTP requests with method, path, status, and latency.
async fn logging_middleware<B>(
    req: Request<B>,
    next: Next<B>,
) -> Result<Response, StatusCode> {
    let method = req.method().clone();
    let path = req.uri().path().to_string();
    let start = std::time::Instant::now();

    // Run the request
    let response = next.run(req).await;

    // Calculate latency
    let latency = start.elapsed().as_secs_f64();
    let status = response.status().as_u16();

    // Log request
    info!(
        "{} {} {} - {:.3}s",
        method,
        path,
        status,
        latency
    );

    Ok(response)
}

/// Rate limiting middleware.
///
/// Prevents DoS attacks by limiting requests per IP address.
/// Returns 429 Too Many Requests if rate limit exceeded.
///
/// Rate limits are configured via environment variables:
/// - `RATE_LIMIT_MAX_REQUESTS`: Maximum requests per window (default: 100)
/// - `RATE_LIMIT_WINDOW_SECS`: Time window in seconds (default: 60)
async fn rate_limit_middleware<B>(
    ConnectInfo(addr): ConnectInfo<SocketAddr>,
    Extension(rate_limiter): Extension<RateLimiter>,
    req: Request<B>,
    next: Next<B>,
) -> Result<Response, StatusCode> {
    let ip = addr.ip().to_string();

    if rate_limiter.check_rate_limit(&ip) {
        // Request allowed
        Ok(next.run(req).await)
    } else {
        // Rate limit exceeded
        warn!("üö´ Rate limit exceeded for IP: {}", ip);
        Err(StatusCode::TOO_MANY_REQUESTS)
    }
}

pub fn router(
    db_pool: PgPool,
    peer_store: crate::network::PeerStore,
    batch_writer: Arc<crate::batch_writer::BatchWriter>,
) -> Router {
    // Initialize rate limiter with configurable limits
    let max_requests = std::env::var("RATE_LIMIT_MAX_REQUESTS")
        .ok()
        .and_then(|v| v.parse::<u32>().ok())
        .unwrap_or(100); // Default: 100 requests per window

    let window_secs = std::env::var("RATE_LIMIT_WINDOW_SECS")
        .ok()
        .and_then(|v| v.parse::<u64>().ok())
        .unwrap_or(60); // Default: 60 second window

    let rate_limiter = RateLimiter::new(max_requests, window_secs);

    info!(
        "üõ°Ô∏è  Rate limiting enabled: {} requests per {} seconds",
        max_requests, window_secs
    );

    // Public routes (no authentication required)
    let public_routes = Router::new()
        .route("/health", get(health))
        .route("/health/detailed", get(health_detailed));

    // Protected routes with rate limiting and authentication
    // Applies BOTH rate limiting and authentication (layers run bottom to top)
    let protected_routes = Router::new()
        .route("/tx/submit", post(submit_tx))
        .route("/mempool", get(get_mempool))
        .route("/tx/:id", get(get_tx_by_id_or_hash))      // accepts uuid or tx_hash; we try both
        .route("/tx/hash/:hash", get(get_tx_by_hash))     // explicit hash lookup
        .route("/block/:id", get(get_block_by_id))        // placeholder route
        .route("/proof/:tx", get(get_proof_by_tx))
        .route("/peers", get(get_peers))
        .layer(middleware::from_fn(auth_middleware))      // Run second (outer layer)
        .layer(middleware::from_fn(rate_limit_middleware)) // Run first (inner layer)
        .layer(Extension(rate_limiter));

    // Combine all routes with global logging middleware
    Router::new()
        .merge(public_routes)
        .merge(protected_routes)
        .layer(middleware::from_fn(logging_middleware))  // Global request logging
        .layer(Extension(db_pool))
        .layer(Extension(peer_store))
        .layer(Extension(batch_writer))  // TPS Optimization: Batch writer for high throughput
}
